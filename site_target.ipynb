{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, confusion_matrix, RocCurveDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from boruta import BorutaPy\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce\n",
    "\n",
    "categorical_columns = ['site', 'cid', 'gasource', 'magecat', 'medu_r2', 'meducat_r2', 'paritycat', 'wttiming',\n",
    "                        'priorcsec', 'anyanc', 'anctri', 'ancvisits', 'ancvisitscat', 'vitcaliron', 'ttvaccine', 'hiv',\n",
    "                        'bpmeas', 'urinetest', 'anyus', 'lb', 'bsex', 'multiple', 'bagmask',\n",
    "                        'bathed', 'antehem', 'posthem', 'hypertensive', 'transverse', 'oblique',\n",
    "                        'breech', 'malp', 'induction', 'infdeliv', 'inffu', 'unplanhosp', 'hospcomp', 'seizures',\n",
    "                        'mantibiotics', 'corticosteroid', 'oxytocics', 'bldtrans', 'dcsuction', 'magsulfate',\n",
    "                        'hysterectomy', 'episiotomy', 'rentown', 'waterimp', 'waternotimp', 'water30min', 'sanitation',\n",
    "                        'floormat', 'cookfuel', 'bicycle', 'motorbike', 'vehicle', 'electricity', 'television',\n",
    "                        'refrigerator', 'computer', 'flipphone', 'smartphone', 'pregout', 'fuout', 'ltfdeliv']\n",
    "\n",
    "numerical_columns = ['gaenrl', 'mage', 'schyears', 'parity', 'numfamily', 'numrooms']\n",
    "\n",
    "target_variable = 'pretermalg'\n",
    "\n",
    "cols_to_read = categorical_columns + numerical_columns + [target_variable]\n",
    "\n",
    "df = pd.read_csv('data.csv', usecols=cols_to_read, dtype=str)\n",
    "df['pretermalg'] = pd.to_numeric(df['pretermalg'], errors='coerce')\n",
    "df = df.dropna(subset=['pretermalg'])\n",
    "df['pretermalg'] = df['pretermalg'].replace({2: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing site: 6\n",
      "[LightGBM] [Info] Number of positive: 2483, number of negative: 18107\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001440 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 202\n",
      "[LightGBM] [Info] Number of data points in the train set: 20590, number of used features: 22\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120593 -> initscore=-1.986831\n",
      "[LightGBM] [Info] Start training from score -1.986831\n",
      "Best Model for site 6: LGBMClassifier with AUC: 0.7403884319952532\n",
      "Processing site: 3\n",
      "[LightGBM] [Info] Number of positive: 2068, number of negative: 13167\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000465 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 157\n",
      "[LightGBM] [Info] Number of data points in the train set: 15235, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.135740 -> initscore=-1.851132\n",
      "[LightGBM] [Info] Start training from score -1.851132\n",
      "Best Model for site 3: LGBMClassifier with AUC: 0.802496805949072\n",
      "Processing site: 11\n",
      "[LightGBM] [Info] Number of positive: 1682, number of negative: 13244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 167\n",
      "[LightGBM] [Info] Number of data points in the train set: 14926, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.112689 -> initscore=-2.063561\n",
      "[LightGBM] [Info] Start training from score -2.063561\n",
      "Best Model for site 11: LGBMClassifier with AUC: 0.7079244298796057\n",
      "Processing site: 9\n",
      "[LightGBM] [Info] Number of positive: 3761, number of negative: 10173\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000657 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 184\n",
      "[LightGBM] [Info] Number of data points in the train set: 13934, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.269915 -> initscore=-0.995052\n",
      "[LightGBM] [Info] Start training from score -0.995052\n",
      "Best Model for site 9: LGBMClassifier with AUC: 0.6431242356968739\n",
      "Processing site: 2\n",
      "[LightGBM] [Info] Number of positive: 2787, number of negative: 12623\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 225\n",
      "[LightGBM] [Info] Number of data points in the train set: 15410, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.180857 -> initscore=-1.510555\n",
      "[LightGBM] [Info] Start training from score -1.510555\n",
      "Best Model for site 2: CatBoostClassifier with AUC: 0.7897367527376014\n",
      "Processing site: 12\n",
      "[LightGBM] [Info] Number of positive: 1758, number of negative: 14119\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000650 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 192\n",
      "[LightGBM] [Info] Number of data points in the train set: 15877, number of used features: 16\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.110726 -> initscore=-2.083345\n",
      "[LightGBM] [Info] Start training from score -2.083345\n",
      "Best Model for site 12: LGBMClassifier with AUC: 0.8000834445415919\n",
      "Processing site: 8\n",
      "[LightGBM] [Info] Number of positive: 1522, number of negative: 12247\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 222\n",
      "[LightGBM] [Info] Number of data points in the train set: 13769, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.110538 -> initscore=-2.085256\n",
      "[LightGBM] [Info] Start training from score -2.085256\n",
      "Best Model for site 8: CatBoostClassifier with AUC: 0.8411360456383744\n",
      "Processing site: 7\n",
      "[LightGBM] [Info] Number of positive: 147, number of negative: 751\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 44\n",
      "[LightGBM] [Info] Number of data points in the train set: 898, number of used features: 2\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.163697 -> initscore=-1.630973\n",
      "[LightGBM] [Info] Start training from score -1.630973\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best Model for site 7: XGBClassifier with AUC: 0.6486486486486486\n"
     ]
    }
   ],
   "source": [
    "for site in df['site'].unique():\n",
    "\tprint(f\"Processing site: {site}\")\n",
    "\t\n",
    "\t# Filter data for the current site\n",
    "\tdf_site = df[df['site'] == site]\n",
    "\t\n",
    "\t# Splitting the data into training and test sets\n",
    "\tX = df_site[categorical_columns + numerical_columns]\n",
    "\ty = df_site[target_variable].astype(int)\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "\t# Imputation for training data\n",
    "\tnum_imputer = SimpleImputer(strategy='mean')\n",
    "\tcat_imputer = SimpleImputer(strategy='constant', fill_value='unknown')\n",
    "\tX_train[numerical_columns] = X_train[numerical_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\tX_test[numerical_columns] = X_test[numerical_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\tX_train[numerical_columns] = num_imputer.fit_transform(X_train[numerical_columns])\n",
    "\tX_train[categorical_columns] = cat_imputer.fit_transform(X_train[categorical_columns])\n",
    "\n",
    "\t# Target Encoding\n",
    "\ttarget_encoder = ce.TargetEncoder()\n",
    "\tX_train_encoded = target_encoder.fit_transform(X_train[categorical_columns], y_train)\n",
    "\n",
    "\t# Scaling\n",
    "\tscaler = StandardScaler()\n",
    "\tX_train_scaled = scaler.fit_transform(X_train[numerical_columns])\n",
    "\n",
    "\t# Combine categorical and numerical data\n",
    "\tX_train_final = np.hstack([X_train_encoded, X_train_scaled])\n",
    "\n",
    "\t# Preprocessing for test data (transform only)\n",
    "\tX_test[numerical_columns] = num_imputer.transform(X_test[numerical_columns])\n",
    "\tX_test[categorical_columns] = cat_imputer.transform(X_test[categorical_columns])\n",
    "\tX_test_encoded = target_encoder.transform(X_test[categorical_columns])\n",
    "\tX_test_scaled = scaler.transform(X_test[numerical_columns])\n",
    "\tX_test_final = np.hstack([X_test_encoded, X_test_scaled])\n",
    "\n",
    "\t# Feature selection\n",
    "\trf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "\tfeat_selector = BorutaPy(rf, n_estimators='auto', random_state=1)\n",
    "\tfeat_selector.fit(X_train_final, y_train)\n",
    "\tselected_features_train = X_train_final[:, feat_selector.support_]\n",
    "\tselected_features_test = X_test_final[:, feat_selector.support_]\n",
    "\n",
    "\t# Model Training and Evaluation\n",
    "\tmodels = {\n",
    "\t\t\"XGBClassifier\": XGBClassifier(),\n",
    "\t\t\"CatBoostClassifier\": CatBoostClassifier(silent=True),\n",
    "\t\t\"LGBMClassifier\": LGBMClassifier(),\n",
    "\t\t\"RandomForestClassifier\": RandomForestClassifier(),\n",
    "\t}\n",
    "\n",
    "\tresults = {}\n",
    "\tfor name, model in models.items():\n",
    "\t\tmodel.fit(selected_features_train, y_train)\n",
    "\t\ty_pred = model.predict_proba(selected_features_test)[:, 1]\n",
    "\t\tauc_score = roc_auc_score(y_test, y_pred)\n",
    "\t\tresults[name] = auc_score\n",
    "\n",
    "\t# Model Comparison\n",
    "\tbest_model_name = max(results, key=results.get)\n",
    "\tprint(f\"Best Model for site {site}: {best_model_name} with AUC: {results[best_model_name]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
