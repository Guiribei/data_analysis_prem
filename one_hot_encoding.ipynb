{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, confusion_matrix, RocCurveDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from boruta import BorutaPy\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['site', 'cid', 'gasource', 'magecat', 'medu_r2', 'meducat_r2', 'paritycat', 'wttiming',\n",
    "                        'priorcsec', 'anyanc', 'anctri', 'ancvisits', 'ancvisitscat', 'vitcaliron', 'ttvaccine', 'hiv',\n",
    "                        'bpmeas', 'urinetest', 'anyus', 'lb', 'bsex', 'multiple', 'bagmask',\n",
    "                        'bathed', 'antehem', 'posthem', 'hypertensive', 'transverse', 'oblique',\n",
    "                        'breech', 'malp', 'induction', 'infdeliv', 'inffu', 'unplanhosp', 'hospcomp', 'seizures',\n",
    "                        'mantibiotics', 'corticosteroid', 'oxytocics', 'bldtrans', 'dcsuction', 'magsulfate',\n",
    "                        'hysterectomy', 'episiotomy', 'rentown', 'waterimp', 'waternotimp', 'water30min', 'sanitation',\n",
    "                        'floormat', 'cookfuel', 'bicycle', 'motorbike', 'vehicle', 'electricity', 'television',\n",
    "                        'refrigerator', 'computer', 'flipphone', 'smartphone', 'pregout', 'fuout', 'ltfdeliv']\n",
    "\n",
    "numerical_columns = ['gaenrl', 'mage', 'schyears', 'parity', 'numfamily', 'numrooms']\n",
    "\n",
    "target_variable = 'pretermalg'\n",
    "\n",
    "cols_to_read = categorical_columns + numerical_columns + [target_variable]\n",
    "\n",
    "df = pd.read_csv('data.csv', usecols=cols_to_read, dtype=str)\n",
    "df['pretermalg'] = pd.to_numeric(df['pretermalg'], errors='coerce')\n",
    "df = df.dropna(subset=['pretermalg'])\n",
    "df['pretermalg'] = df['pretermalg'].replace({2: 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[categorical_columns + numerical_columns], \n",
    "                                                    df[target_variable], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=1)\n",
    "\n",
    "# Imputation for training data\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='unknown')\n",
    "\n",
    "# Ensure all numerical columns are numeric and contain no non-numeric values\n",
    "X_train[numerical_columns] = X_train[numerical_columns].apply(pd.to_numeric, errors='coerce')\n",
    "X_test[numerical_columns] = X_test[numerical_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Apply imputers\n",
    "X_train[numerical_columns] = num_imputer.fit_transform(X_train[numerical_columns])\n",
    "X_train[categorical_columns] = cat_imputer.fit_transform(X_train[categorical_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_train_cat_encoded = encoder.fit_transform(X_train[categorical_columns])\n",
    "X_test_cat_encoded = encoder.transform(X_test[categorical_columns])\n",
    "\n",
    "# Combine encoded categorical data with numerical data\n",
    "X_train_final = np.hstack([X_train_cat_encoded, X_train[numerical_columns]])\n",
    "X_test_final = np.hstack([X_test_cat_encoded, X_test[numerical_columns]])\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_final_scaled = scaler.fit_transform(X_train_final)\n",
    "X_test_final_scaled = scaler.transform(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for test data (transform only)\n",
    "# Apply numerical imputer\n",
    "X_test[numerical_columns] = num_imputer.transform(X_test[numerical_columns])\n",
    "\n",
    "# Apply categorical imputer\n",
    "X_test[categorical_columns] = cat_imputer.transform(X_test[categorical_columns])\n",
    "\n",
    "# Apply one-hot encoding to categorical columns\n",
    "X_test_cat_encoded = encoder.transform(X_test[categorical_columns])\n",
    "\n",
    "# Combine encoded categorical data with numerical data\n",
    "X_test_final = np.hstack([X_test_cat_encoded, X_test[numerical_columns]])\n",
    "\n",
    "# Scaling the combined data\n",
    "X_test_final_scaled = scaler.transform(X_test_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "feat_selector = BorutaPy(rf, n_estimators='auto', random_state=1)\n",
    "feat_selector.fit(X_train_final, y_train)\n",
    "selected_features_train = X_train_final[:, feat_selector.support_]\n",
    "selected_features_test = X_test_final[:, feat_selector.support_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16163, number of negative: 94479\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 393\n",
      "[LightGBM] [Info] Number of data points in the train set: 110642, number of used features: 106\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.146084 -> initscore=-1.765653\n",
      "[LightGBM] [Info] Start training from score -1.765653\n",
      "Best Model: CatBoostClassifier with AUC: 0.7802893026922387\n"
     ]
    }
   ],
   "source": [
    "# Model Training and Evaluation\n",
    "models = {\n",
    "    \"XGBClassifier\": XGBClassifier(),\n",
    "    \"CatBoostClassifier\": CatBoostClassifier(silent=True),\n",
    "    \"LGBMClassifier\": LGBMClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(selected_features_train, y_train)\n",
    "    y_pred = model.predict_proba(selected_features_test)[:, 1]\n",
    "    auc_score = roc_auc_score(y_test, y_pred)\n",
    "    results[name] = auc_score\n",
    "\n",
    "# Model Comparison\n",
    "best_model_name = max(results, key=results.get)\n",
    "print(f\"Best Model: {best_model_name} with AUC: {results[best_model_name]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
